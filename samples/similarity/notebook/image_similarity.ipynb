{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity: AML Package for Computer Vision\n",
    "A large number of problems in the computer vision domain can be solved by ranking images according to their similarity. For example, retail companies want to show customers products which are similar to the ones bought in the past. Or companies with large amounts of data want to organize and search their images effectively.\n",
    "\n",
    "This notebook shows how the AML Package for Computer vision can be used to train, evaluate, and deploy an image similarity model. Example images and annotations are provided, but the reader can bring their own dataset and train their own unique ranker. Currently, CNTK is used as the deep learning framework. Training is peformed locally on a GPU powered machine ([Deep Learning VM](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning?tab=Overview)), and deployment uses the Azure ML Operationalization CLI.\n",
    "\n",
    "It is encouraged to first try the **Image Classification** tutorial before running this tutorial as many features from Image Classification are used.\n",
    "\n",
    "The following steps are performed:\n",
    "1. Dataset Creation\n",
    "2. Image Pairs\n",
    "3. Model Definition and Training\n",
    "4. Evaluation and Visualzation\n",
    "5. Webservice Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Our approach to measure image similarity is visualized at a high level as shown by the diagram below:\n",
    "\n",
    "<img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/pipeline.jpg?raw=true\" width=800>\n",
    "\n",
    "* Given two images, we want to measure the visual distance between them. For the purpose of this tutorial distance and similarity are used interchangebly (since similarity can be computed by simply taking the negative of the distance). \n",
    "* Optionally, in a pre-processing step, one can detect the object-of-interest and crop the image to that area. Please see the Object Detection notebook for how to train and evaluate such a model based on an approach called Faster-RCNN. \n",
    "* Each image is then represented using the output of a DNN which was pre-trained on millions of images. The input to the DNN is simply the image itself, and the output is the penultimate layer (512 floating point values for the ResNet18 model). \n",
    "* These 512-floats image representations are then scaled to each have an L2 norm of one, and the visual distance between two images is defined as a function of their respective 512-floats vectors. Possible distance metrics include the L1 or the L2 norm. The advantage of these metrics is that they are non-parametric and therefore do not require any training. The disadvantage however is that each of the 512 dimensions is assumed to carry the same amount of information which in practice is not true. Hence, we use a Linear SVM to train a weighted L2 distance, which can give a higher weight to informative dimensions, and vice versa down-weight dimensions with little or no information.\n",
    "* Finally, the output of the system is the desired visual similarity for the given image pair.\n",
    "\n",
    "Using the output of a DNN is powerful and is shown to give good results on a wide variety of tasks. However, better results can be achieved through fine-tuning the network before computing the image representations. This approach is known as [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning): starting with an original model trained on a large dataset of generic images, it is then fine-tuned in this tutorial using a small set of images from our own dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, json, shutil, cntk\n",
    "import cvtk\n",
    "from cvtk.utils import Constants\n",
    "from cvtk.core import Context, ClassificationDataset, Image, Label, Splitter, CNTKTLModel\n",
    "from cvtk.core.ranker import ImagePairs, ImageSimilarityMetricRanker, ImageSimilarityLearnerRanker, ImageSimilarityRandomRanker, RankerEvaluation\n",
    "from cvtk.utils.ranker_utils import visualize_ranked_images\n",
    "from cvtk.augmentation import augment_dataset\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Creation\n",
    "\n",
    "The recommended way to generate a Dataset object in CVTK is by providing the root directory of the images on the local disk. This directory has to follow the same general structure as the tableware dataset in CVTK's image classification notebook, ie. contain sub-directories with the actual images:\n",
    "- root\n",
    "    - label1\n",
    "    - label2\n",
    "    - ...\n",
    "    - labeln\n",
    "  \n",
    "Using a different dataset is therefore as easy as changing the root path `dataset_location` in the code below to point at different images, and to set `dataset_name` to any user-chosen string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the local storage context\n",
    "Note AML has a limit of 25 MB, so we will set the CVTK outputs directory to be outside of the workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set storage context.\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)\n",
    "\n",
    "# If the user wants to use a local context without a json file run the lines below\n",
    "#if 'AZUREML_NATIVE_SHARE_DIRECTORY' not in os.environ:\n",
    "#    os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] = './share'\n",
    "#context = Context.get_global_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Dataset and Split\n",
    "We will download a small upper body clothing texture dataset of around 330 images, annotated in one of 3 different textures: dotted, striped, leopard. The figure below shows examples for the attributes of the dotted (left two columns), striped (middle two columns), and leopard (right two columns). It is important to note that the annotations were done according to the upper body clothing item. The ranker needs to learn to focus on the relevant part of the image and to ignore all other areas (e.g. pants, shoes). \n",
    "\n",
    "|<h3><center>Dotted</center></h3>|<h3><center>Striped</center></h3>|<h3><center>Leopard</center></h3>|\n",
    "|:-------------:|:-------------:|:-----:|\n",
    "| <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/examples_dotted.jpg?raw=true\" width=250> | <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/examples_striped.jpg?raw=true\" width=250> | <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/examples_leopard.jpg?raw=true\" width=250> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset name and location\n",
    "dataset_name = \"fashion\"\n",
    "dataset_location = os.path.join(Context.get_global_context().storage.outputs_path, \"data\", dataset_name)\n",
    "\n",
    "# A dataset location can also be specfied as shown here\n",
    "# dataset_location=r\"../classification/sample_data/imgs_recycling\" \n",
    "# dataset_name = \"my_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download around 330 images. This cell only needs to be executed once.\n",
    "import download_images\n",
    "print(\"Downloading images to: \" + dataset_location)\n",
    "download_images.download_all(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are assigned for either training or for testing - this split is mutually exclusive. Here we use a ratio of 0.5, 50% of the images from each attribute are assigned to training, and 50% to testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassificationDataset.create_from_dir(dataset_name, dataset_location)\n",
    "print(\"Dataset consists of {} images with {} labels.\".format(len(dataset.images), len(dataset.labels)))\n",
    "# Split the data into train and test\n",
    "train_set, test_set = dataset.split(train_size = .5, random_state=1, stratify=\"label\")\n",
    "print(\"Number of original training images = {}.\".format(train_set.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Image Pairs Generation\n",
    "\n",
    "Image pairs are used to train and evaluate the image ranker. We select up to `num_train_sets=60` query images from each of the 3 attributes. Each query image is paired with one image from the same attribute, and up to\n",
    "`num_ref_images_per_set=50` images from other attributes. This leads to a maximum of 60\\*50 = 3000 mostly negative image pairs. For testing the same approach is used. More details of the implementation can be seen in the documenation.\n",
    "\n",
    "Shown below are randomly generated image pairs for a given query image in the top row: (left) positive pair since the clothing texture in both images is dotted; (middle and right) negative pairs where the images have different textures. \n",
    "\n",
    "\n",
    "|<h3><center>Positive</center></h3>|<h3><center>Negative</center></h3>|<h3><center>Negative</center></h3>|\n",
    "|:-------------:|:-------------:|:-----:|\n",
    "| <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/example_pair_pos.jpg?raw=true\" width=150> | <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/example_pair_neg2.jpg?raw=true\" width=150> | <img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/example_pair_neg1.jpg?raw=true\" width=150> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get errors running this due to downloads try reducing num_different_label\n",
    "num_train_sets = 60\n",
    "num_test_sets = 60\n",
    "num_ref_images_per_set = 50\n",
    "train_pairs = ImagePairs(train_set, num_train_sets, num_ref_images_per_set)\n",
    "print('There are {} sets of image pairs generated for all labels from training data.'.format(len(train_pairs.image_sets)))\n",
    "test_pairs = ImagePairs(test_set, num_test_sets, num_ref_images_per_set)\n",
    "print('There are {} sets of image pairs generated for all labels from training data.'.format(len(test_pairs.image_sets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Definition and Training: \n",
    "### Load pre-trained DNN model and optionally refine it\n",
    "This code refines a pre-trained DNN using the training set from previous steps. Note that this can be slow even with a GPU. Hence, optionally, the pre-trained DNN can be used as-is which however will produce suboptimal image representations, and reduce ranker accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_DNN = True # Use the pretrained model as-is or refine\n",
    "model = CNTKTLModel(train_set.labels, class_map = {i: l.name for i, l in enumerate(dataset.labels)}, base_model_name='ResNet18_ImageNet_CNTK')\n",
    "if refine_DNN:\n",
    "    model.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Similarity Ranker\n",
    "Instantiate (and if required train) the image similarity ranker. These rankers internally compare two images using e.g. the L2 distance of the 512-floats image representations, or an SVM which was trained to score how similar two images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_method = \"l2\" # Options: \"random\", \"L2\", \"svm\"\n",
    "\n",
    "if similarity_method == \"random\":\n",
    "    ranker = ImageSimilarityRandomRanker()\n",
    "elif similarity_method == \"l2\":\n",
    "    ranker = ImageSimilarityMetricRanker(model, metric=\"l2\")\n",
    "elif similarity_method == \"svm\":\n",
    "    from sklearn.svm import LinearSVC\n",
    "    svm_learner = LinearSVC(C = 0.01) # SVM-defined weighted L2-distance. Need to train, but this is fast.\n",
    "    ranker = ImageSimilarityLearnerRanker(model, learner=svm_learner)\n",
    "\n",
    "# Train the ranker, random and L2 do not need training and .train() will do nothing\n",
    "ranker.train(train_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Quantitative evaluation is performed using ImagePairs, where each query image is paired with 1 positive and 50 negative images. These 51 reference images are sorted using their distance to the query image. Then the rank of the positive image within the 51 images is computed. Rank 1 corresponds to the best possible result, rank 51 to the worst. Random guessing would on average produce a rank of 25. \n",
    "\n",
    "The diagram below, after sorting, shows where the postive image has rank of 3 (note that this example uses 100 negative images):\n",
    "<img src=\"https://github.com/Azure/ImageSimilarityUsingCntk/blob/master/doc/example_ranking.jpg?raw=true\" width=800> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = RankerEvaluation(ranker, test_pairs)\n",
    "acc_top_1 = re.compute_accuracy(top_n = 1)\n",
    "acc_top_5 = re.compute_accuracy(top_n = 5)\n",
    "mean_rank = re.compute_mean_rank()\n",
    "median_rank = re.compute_median_rank()\n",
    "\n",
    "print('Top 1 accuracy: {} %'.format(str(acc_top_1)))\n",
    "print('Top 5 accuracy: {} %'.format(str(acc_top_5)))\n",
    "print('Mean rank: {}'.format(str(mean_rank)))\n",
    "print('Median rank: {}'.format(str(median_rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_plot = re.top_n_acc_plot(n=32, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results\n",
    "Given an image pair, we can compare its query image (left) to all reference images and show the reference image with lowest distance (middle), as well as the positive reference image in the image pair (right). In the best case scenario where the positive image has the lowest distance, both the middle and right will show the same image. \n",
    "\n",
    "An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualized_results_plots = re.visualize_results(n=2, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Related Images Given a Query Image\n",
    "Here we can visualize the related images given a query image. If using a deployed cluster, you can also pass the serialized output and get the related images using the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "ranker.set_reference_data(test_set) # need to set the reference set to test against\n",
    "image_path = test_set.images[0].storage_path # take an example image to score against\n",
    "input_image = cv2.imread(image_path) # requires an image read in an OpenCV format\n",
    "similar_images_plots = re.visualize_similar_images(input_image, n=4, visualize=True) # Actually does the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing as web-service \n",
    "### Webservice Deployment\n",
    "\n",
    "\n",
    "<b>Prerequisites:</b> \n",
    "Please the check the **Prerequisites** section of our deployment notebook to set up your deployment CLI. You only need to set it up once for all your deployments. More deployment related topics including IoT Edge deployment can be found in the deployment notebook.\n",
    "       \n",
    "<b>Deployment API:</b>\n",
    "\n",
    "> **Examples:**\n",
    "- ```deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=dnn_model, aml_env=\"cluster\")``` # create deployment object\n",
    "- ```deploy_obj.deploy()``` # deploy web service\n",
    "- ```deploy_obj.status()``` # get status of deployment\n",
    "- ```deploy_obj.score_image(local_image_path_or_image_url)``` # score an image\n",
    "- ```deploy_obj.delete()``` # delete the web service\n",
    "- ```deploy_obj.build_docker_image()``` # build docker image without creating webservice\n",
    "- ```AMLDeployment.list_deployment()``` # list existing deployment\n",
    "- ```AMLDeployment.delete_if_service_exist(deployment_name)``` # delete if the service exists with the deployment name\n",
    "\n",
    "<b>Deployment management with portal:</b>\n",
    "\n",
    "You can go to [Azure portal](https://ms.portal.azure.com/) to track and manage your deployments. From Azure portal, find your Machine Learning Model Management account page (You can search for your model management account name). Then go to: the model management account page->Model Management->Services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the deployment by specifying an AMLDeployment object. Here the ranker is passed to the deployment object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### OPTIONAL - Interactive CLI setup helper ###### \n",
    "# # Interactive CLI setup helper, including model management account and deployment environment.\n",
    "# # If you haven't setup you CLI before or if you want to change you CLI settings, you can use this block to help you interactively.\n",
    "# # UNCOMMENT THE FOLLOWING LINES IF YOU HAVE NOT CREATED OR SET THE MODEL MANAGEMENT ACCOUNT AND DEPLOYMENT ENVIRONMENT\n",
    "\n",
    "# from azuremltkbase.deployment import CliSetup\n",
    "# CliSetup().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional. Persist you model on disk and reuse it later for deployment. \n",
    "# from cvtk import CNTKTLModel, Context\n",
    "# from cvtk.core.ranker import ImageSimilarityMetricRanker, ImageSimilarityLearnerRanker, ImageSimilarityRandomRanker, RankerEvaluation\n",
    "# import os\n",
    "# save_model_path = os.path.join(Context.get_global_context().storage.persistent_path, \"saved_ranker.model\")\n",
    "# # Save model to disk\n",
    "# ranker.save(save_model_path)\n",
    "# # Load model from disk\n",
    "# ranker = ImageSimilarityLearnerRanker.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "# set deployment name\n",
    "deployment_name = \"imagesimilarity\"\n",
    "\n",
    "# Set a reference dataset\n",
    "ranker.set_reference_data(test_set)\n",
    "\n",
    "# Create deployment object\n",
    "# It will use the current deployment environment (you can check it with CLI command \"az ml env show\").\n",
    "deploy_obj = AMLDeployment(deployment_name=deployment_name, aml_env=\"cluster\", associated_DNNModel=ranker, replicas=1)\n",
    "\n",
    "# Alternatively, you can provide azure machine learning deployment cluster name (environment name) and resource group name\n",
    "# to deploy your model. It will use the provided cluster to deploy. To do that, please uncomment the following lines to create \n",
    "# the deployment object.\n",
    "\n",
    "# azureml_rscgroup = \"<resource group>\"\n",
    "# cluster_name = \"<cluster name>\"\n",
    "# deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=ranker,\n",
    "#                            aml_env=\"cluster\", cluster_name=cluster_name, resource_group=azureml_rscgroup, replicas=1)\n",
    "\n",
    "# Check if the webservice exists, if yes remove it first.\n",
    "if deploy_obj.is_existing_service():\n",
    "    AMLDeployment.delete_if_service_exist(deployment_name)\n",
    "    \n",
    "# create the webservice\n",
    "print(\"Deploying to Azure cluster...\")\n",
    "deploy_obj.deploy()\n",
    "print(\"Deployment DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webservice comsumption\n",
    "\n",
    "Once you created the webservice, you can score images with the deployed webservice. You have several options:\n",
    "\n",
    "   - You can directly score the webservice with the deployment object with: deploy_obj.score_image(image_path_or_url) \n",
    "   - Or, you can use the Service endpoin url and Serivce key (None for local deployment) with: AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "   - Form your http requests directly to score the webservice endpoint (For advanced users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with existing deployment object\n",
    "```\n",
    "deploy_obj.score_image(image_path_or_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with existing deployment object\n",
    "\n",
    "# Score local image with file path\n",
    "print(\"Score local image with file path\")\n",
    "image_path_or_url = test_set.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n",
    "# If you want to view the similar images\n",
    "#re.visualize_similar_images_from_json(serialized_result_in_json)\n",
    "\n",
    "# Score image url and remove image resizing\n",
    "print(\"Score image url\")\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time image scoring\n",
    "import timeit\n",
    "\n",
    "num_images = 10\n",
    "for img_index, img_obj in enumerate(test_set.images[:num_images]):\n",
    "    print(\"Calling API for image {} of {}: {}...\".format(img_index, num_images, img_obj.name))\n",
    "    tic = timeit.default_timer()\n",
    "    return_json = deploy_obj.score_image(img_obj.storage_path, image_resize_dims=[224,224])\n",
    "    print(\"   Time for API call: {:.2f} seconds\".format(timeit.default_timer() - tic))\n",
    "    print(return_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with service endpoint url and service key\n",
    "```\n",
    "    AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related classes and functions\n",
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "service_endpoint_url = \"\" # please replace with your own service url\n",
    "service_key = \"\" # please replace with your own service key\n",
    "# score local image with file path\n",
    "image_path_or_url = test_set.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)\n",
    "\n",
    "# score image url\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score endpoint with http request directly\n",
    "Following is some example code to form the http request directly in Python. You can do it in other programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_with_http(image, service_endpoint_url, service_key=None, parameters={}):\n",
    "    \"\"\"Score local image with http request\n",
    "\n",
    "    Args:\n",
    "        image (str): Image file path\n",
    "        service_endpoint_url(str): web service endpoint url\n",
    "        service_key(str): Service key. None for local deployment.\n",
    "        parameters (dict): Additional request paramters in dictionary. Default is {}.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        str: serialized result \n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    if service_key is None:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "    else:\n",
    "        headers = {'Content-Type': 'application/json',\n",
    "                   \"Authorization\": ('Bearer ' + service_key)}\n",
    "    payload = []\n",
    "    encoded = None\n",
    "    \n",
    "    # Read image\n",
    "    with open(image,'rb') as f:\n",
    "        image_buffer = BytesIO(f.read()) ## Getting an image file represented as a BytesIO object\n",
    "        \n",
    "    # Convert your image to base64 string\n",
    "    # image_in_base64 : \"b'{base64}'\"\n",
    "    encoded = base64.b64encode(image_buffer.getvalue())\n",
    "    image_request = {\"image_in_base64\": \"{0}\".format(encoded), \"parameters\": parameters}\n",
    "    payload.append(image_request)\n",
    "    body = json.dumps(payload)\n",
    "    r = requests.post(service_endpoint_url, data=body, headers=headers)\n",
    "    try:\n",
    "        result = json.loads(r.text)\n",
    "        json.loads(result[0])\n",
    "    except:\n",
    "        raise ValueError(\"Incorrect output format. Result cant not be parsed: \" + r.text)\n",
    "    return result[0]\n",
    "\n",
    "# Test with images\n",
    "image = test_set.images[0].storage_path # A local image file\n",
    "score_image_with_http(image, service_endpoint_url, service_key) # Local scoring the service_key is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse serialized result from webservice\n",
    "The result from the webserice is in json string. You can parse it the with different DNN model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse result from json string\n",
    "parsed_result = ImageSimilarityLearnerRanker.parse_serialized_result(serialized_result_in_json)\n",
    "print(\"Parsed result:\", parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2018 Microsoft. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
