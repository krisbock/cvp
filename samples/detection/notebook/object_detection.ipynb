{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Detection: AML Package for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "Object Detection is one of the main problems in Computer Vision. Traditionally, this required expert knowledge to identify and implement so called “features” that highlight the position of objects in the image. Starting in 2012 with the famous [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), Deep Neural Networks are used to automatically find these features.\n",
    "\n",
    "This notebook shows how the Azure Machine Learning Package for Computer Vision can be used to train, evaluate, and deploy a [Faster R-CNN](https://arxiv.org/abs/1506.01497) object detection model. The Computer Vision Toolkit (CVTK) makes it easy to perform all these steps, and internally uses [Tensorflow's implementation](https://arxiv.org/abs/1611.10012) of Faster R-CNN. Faster R-CNN was shown to produce state-of-the-art results for Pascal VOC, one of the main object detection challenges in the field. For more information see the [Tensorflow object detection website](https://github.com/tensorflow/models/tree/master/research/object_detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Annotation\n",
    "\n",
    "Manually annotated object locations are required to train and evaluate an object detector. One excellent UI to help drawing bounding boxes is [LabelImg](https://tzutalin.github.io/labelImg) which can be installed using [windows_v1.6.0.zip](https://www.dropbox.com/s/tq7zfrcwl44vxan/windows_v1.6.0.zip?dl=1). LabelImg writes an xml-file per image in Pascal-VOC format, which can be read into CVTK (see below). \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/9322661/38701920-2c85c962-3e6d-11e8-8cad-b4578336b231.JPG\" width=800>\n",
    "\n",
    "### Example Dataset\n",
    "For this demo, a dataset of grocery items inside refrigerators is provided, consisting of 30 images, and 8 classes (e.g. water, orange, mushroom, etc). These images can be see in the folder \"../sample_data/foods\". For each jpg image in the folder \"../sample_data/foods/train/JPEGImages\", an annotation xml-file with similar name exists in  \"../sample_data/foods/train/Annotations\". These images and bounding box annotations will be loaded in the next section.\n",
    "\n",
    "The Figure below shows the recommended folder structure. \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/9322661/38628125-a363aaa8-3d7e-11e8-884d-e53a53a8da90.JPG\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "### Step 1: Dataset creation\n",
    "\n",
    "Create a CVTK dataset object which consists of a set of images, with their respective bounding box annotations. In the example below, we will read in the refrigerator images which are provided in the \"../sample_data/foods/training\" folder. Note that only JPEG images are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, time\n",
    "from cvtk.core import Context, ObjectDetectionDataset, TFFasterRCNN\n",
    "from cvtk.utils import detection_utils\n",
    "\n",
    "# Disable printing of logging messages\n",
    "from azuremltkbase.logging import ToolkitLogger\n",
    "ToolkitLogger.getInstance().setEnabled(False)\n",
    "\n",
    "# Initialize the context object\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Display the images\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_folder = \"../sample_data/foods/train\"\n",
    "data_train = ObjectDetectionDataset.create_from_dir(dataset_name='training_dataset', data_dir=image_folder,\n",
    "                                                    annotations_dir=\"Annotations\", image_subdirectory='JPEGImages')\n",
    "\n",
    "# Show some statistics of the training image, and also give one example of the ground truth rectangle annotations\n",
    "data_train.print_info()\n",
    "_ = data_train.images[2].visualize_bounding_boxes(image_size = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Define a model\n",
    "\n",
    "Various parameters can be provided when defining a model. The meaning of these parameters, as well as the parameters used for training (see next section) can be found in either CVTK's API docs, or on the [Tensorflow object detection website](https://github.com/tensorflow/models/tree/master/research/object_detection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.0       # Threshold on the detection score, use to discard lower-confidence detections.\n",
    "max_total_detections = 300  # Maximum number of detections. A high value will slow down training but might increase accuracy.\n",
    "my_detector = TFFasterRCNN(labels=data_train.labels, \n",
    "                           score_threshold=score_threshold, \n",
    "                           max_total_detections=max_total_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now train our object detector. This requires a GPU, and for the refrigerator datasets can take up to 5 minutes. The number or training steps in the code is set to 350, so that training runs more quickly (~5minutes). In practice, one should set it to at least 10 times the number of images in the training set.\n",
    "\n",
    "Two key parameters for training are number of steps and learning rate(s).The argument num_steps can be used to specify the number of minibatches used to train the model. Since the minibatch size is set to 1 in this release, it equals the number of images considered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tensorboard --logdir={}\".format(my_detector.train_dir))\n",
    "\n",
    "# to get good results, use a larger value for num_steps, e.g., 5000.\n",
    "num_steps = 350\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "start_train = time.time()\n",
    "my_detector.train(dataset=data_train, num_steps=num_steps, \n",
    "                  initial_learning_rate=learning_rate)\n",
    "end_train = time.time()\n",
    "print(end_train-start_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can be used to visualize the training progress. TensorBoard events are located in the folder specified by the model object's train_dir attribute. Copy the printout that starts with 'tensorboard --logdir' to a command line to view TensorBoard. Then copy the URL from the command window to a web browser to view it. Once you open the TensorBoard, you should see an window like the following screenshot. If Firefox does not render TensorBoard correctly, please try other browsers. \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/9322661/39355941-a3785452-49dc-11e8-973f-080044feaa32.JPG\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create an evaluation dataset, and use this to compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "image_folder = \"../sample_data/foods/test\"\n",
    "data_val = ObjectDetectionDataset.create_from_dir(dataset_name='val_dataset', data_dir=image_folder)\n",
    "eval_result = my_detector.evaluate(dataset=data_val)\n",
    "\n",
    "# print out the performance metric values\n",
    "for label_obj in data_train.labels:\n",
    "    label = label_obj.name\n",
    "    key = 'PASCAL/PerformanceByCategory/AP@0.5IOU/' + label\n",
    "    print('{0: <15}: {1: <3}'.format(label, round(eval_result[key], 2)))\n",
    "    \n",
    "print('{0: <15}: {1: <3}'.format(\"overall:\", round(eval_result['PASCAL/Precision/mAP@0.5IOU'], 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation results can also be viewed using TensorBoard. For instance, copy the following printout to a command line to view TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorboard --logdir={} --port=8008\".format(my_detector.eval_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, you can compute the accuracy of the model on the training set. This is a sanity check to make sure training converged to a good solution. Note that the accuracy on the training set after successful training is often close to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "### Step 5: Score an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = data_val.images[0].storage_path\n",
    "scores = my_detector.score(image_path)\n",
    "path_save = out_root_path + \"/scored_images/scored_image_preloaded.jpg\"\n",
    "ax = detection_utils.visualize(image_path, scores, image_size=(8, 12))\n",
    "path_save_dir = os.path.dirname(os.path.abspath(path_save))\n",
    "os.makedirs(path_save_dir, exist_ok=True)\n",
    "ax.get_figure().savefig(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 6: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "save_model_path = out_root_path + \"/frozen_model/faster_rcnn.model\" # Please save your model to outside of your AML workbench project folder because of the size limit of AML project\n",
    "my_detector.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 7: Score an image with the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to be loaded once for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_detector_loaded = TFFasterRCNN.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is loaded, it can be used to score an image or a list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_dict = my_detector_loaded.score(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print out the detected objects with scores above 0.5, including labels, scores, and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = dict((v,k) for k,v in my_detector.class_map.items())\n",
    "n_obj = 0\n",
    "for i in range(detections_dict['num_detections']):\n",
    "    if detections_dict['detection_scores'][i] > 0.5:\n",
    "        n_obj += 1\n",
    "        print(\"Object {}: label={:11}, score={:.2f}, location=(top: {:.2f}, left: {:.2f}, bottom: {:.2f}, right: {:.2f})\".format(\n",
    "            i, look_up[detections_dict['detection_classes'][i]], \n",
    "            detections_dict['detection_scores'][i], \n",
    "            detections_dict['detection_boxes'][i][0],\n",
    "            detections_dict['detection_boxes'][i][1], \n",
    "            detections_dict['detection_boxes'][i][2],\n",
    "            detections_dict['detection_boxes'][i][3]))    \n",
    "        \n",
    "print(\"\\nFound {} objects in image {}.\".format(n_obj, image_path))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 8: Visualize the scored image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "path_save = out_root_path + \"/scored_images/scored_image_frozen_graph.jpg\"\n",
    "ax = detection_utils.visualize(image_path, detections_dict, path_save=path_save, image_size=(8, 12))\n",
    "# ax.get_figure() # use this code extract the returned image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Webservice Deployment\n",
    "\n",
    "\n",
    "<b>Prerequisites:</b> \n",
    "Please the check the **Prerequisites** section of our deployment notebook to set up your deployment CLI. You only need to set it up once for all your deployments. More deployment related topics including IoT Edge deployment can be found in the deployment notebook.\n",
    "       \n",
    "<b>Deployment API:</b>\n",
    "\n",
    "> **Examples:**\n",
    "- ```deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=dnn_model, aml_env=\"cluster\")``` # create deployment object\n",
    "- ```deploy_obj.deploy()``` # deploy web service\n",
    "- ```deploy_obj.status()``` # get status of deployment\n",
    "- ```deploy_obj.score_image(local_image_path_or_image_url)``` # score an image\n",
    "- ```deploy_obj.delete()``` # delete the web service\n",
    "- ```deploy_obj.build_docker_image()``` # build docker image without creating webservice\n",
    "- ```AMLDeployment.list_deployment()``` # list existing deployment\n",
    "- ```AMLDeployment.delete_if_service_exist(deployment_name)``` # delete if the service exists with the deployment name\n",
    "\n",
    "<b>Deployment management with portal:</b>\n",
    "\n",
    "You can go to [Azure portal](https://ms.portal.azure.com/) to track and manage your deployments. From Azure portal, find your Machine Learning Model Management account page (You can search for your model management account name). Then go to: the model management account page->Model Management->Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### OPTIONAL - Interactive CLI setup helper ###### \n",
    "# # Interactive CLI setup helper, including model management account and deployment environment.\n",
    "# # If you haven't setup you CLI before or if you want to change you CLI settings, you can use this block to help you interactively.\n",
    "# # UNCOMMENT THE FOLLOWING LINES IF YOU HAVE NOT CREATED OR SET THE MODEL MANAGEMENT ACCOUNT AND DEPLOYMENT ENVIRONMENT\n",
    "\n",
    "# from azuremltkbase.deployment import CliSetup\n",
    "# CliSetup().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "# set deployment name\n",
    "deployment_name = \"wsdeployment\"\n",
    "\n",
    "# Create deployment object\n",
    "# It will use the current deployment environment (you can check it with CLI command \"az ml env show\").\n",
    "deploy_obj = AMLDeployment(deployment_name=deployment_name, aml_env=\"cluster\", associated_DNNModel=my_detector, replicas=1)\n",
    "\n",
    "# Alternatively, you can provide azure machine learning deployment cluster name (environment name) and resource group name\n",
    "# to deploy your model. It will use the provided cluster to deploy. To do that, please uncomment the following lines to create \n",
    "# the deployment object.\n",
    "\n",
    "# azureml_rscgroup = \"<resource group>\"\n",
    "# cluster_name = \"<cluster name>\"\n",
    "# deploy_obj = AMLDeployment(deployment_name=deployment_name, associated_DNNModel=my_detector,\n",
    "#                            aml_env=\"cluster\", cluster_name=cluster_name, resource_group=azureml_rscgroup, replicas=1)\n",
    "\n",
    "# Check if the deployment name exists, if yes remove it first.\n",
    "if deploy_obj.is_existing_service():\n",
    "    AMLDeployment.delete_if_service_exist(deployment_name)\n",
    "    \n",
    "# create the webservice\n",
    "print(\"Deploying to Azure cluster...\")\n",
    "deploy_obj.deploy()\n",
    "print(\"Deployment DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Webservice consumption\n",
    "\n",
    "Once you created the webservice, you can score images with the deployed webservice. You have several options:\n",
    "\n",
    "   - You can directly score the webservice with the deployment object with: deploy_obj.score_image(image_path_or_url) \n",
    "   - Or, you can use the Service endpoin url and Serivce key (None for local deployment) with: AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "   - Form your http requests directly to score the webservice endpoint (For advanced users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with existing deployment object\n",
    "```\n",
    "deploy_obj.score_image(image_path_or_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score with existing deployment object\n",
    "\n",
    "# Score local image with file path\n",
    "print(\"Score local image with file path\")\n",
    "image_path_or_url = data_train.images[0].storage_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])\n",
    "\n",
    "# Score image url and remove image resizing\n",
    "print(\"Score image url\")\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time image scoring\n",
    "import timeit\n",
    "\n",
    "num_images = 3\n",
    "for img_index, img_obj in enumerate(data_train.images[:num_images]):\n",
    "    print(\"Calling API for image {} of {}: {}...\".format(img_index, num_images, img_obj.name))\n",
    "    tic = timeit.default_timer()\n",
    "    return_json = deploy_obj.score_image(img_obj.storage_path, image_resize_dims=[224,224])\n",
    "    print(\"   Time for API call: {:.2f} seconds\".format(timeit.default_timer() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score with service endpoint url and service key\n",
    "```\n",
    "    AMLDeployment.score_existing_service_with_image(image_path_or_url, service_endpoint_url, service_key=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related classes and functions\n",
    "from cvtk.operationalization import AMLDeployment\n",
    "\n",
    "service_endpoint_url = \"http://xxx\" # please replace with your own service url\n",
    "service_key = \"xxx\" # please replace with your own service key\n",
    "\n",
    "# score image url\n",
    "image_path_or_url = \"https://cvtkdata.blob.core.windows.net/publicimages/microsoft_logo.jpg\"\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = AMLDeployment.score_existing_service_with_image(image_path_or_url,service_endpoint_url, service_key = service_key, image_resize_dims=[224,224])\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score endpoint with http request directly\n",
    "Following is some example code to form the http request directly in Python. You can do it in other programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_with_http(image, service_endpoint_url, service_key=None, parameters={}):\n",
    "    \"\"\"Score local image with http request\n",
    "\n",
    "    Args:\n",
    "        image (str): Image file path\n",
    "        service_endpoint_url(str): web service endpoint url\n",
    "        service_key(str): Service key. None for local deployment.\n",
    "        parameters (dict): Additional request paramters in dictionary. Default is {}.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        str: serialized result \n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    import json\n",
    "\n",
    "    if service_key is None:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "    else:\n",
    "        headers = {'Content-Type': 'application/json',\n",
    "                   \"Authorization\": ('Bearer ' + service_key)}\n",
    "    payload = []\n",
    "    encoded = None\n",
    "    \n",
    "    # Read image\n",
    "    with open(image,'rb') as f:\n",
    "        image_buffer = BytesIO(f.read()) ## Getting an image file represented as a BytesIO object\n",
    "        \n",
    "    # Convert your image to base64 string\n",
    "    # image_in_base64 : \"b'{base64}'\"\n",
    "    encoded = base64.b64encode(image_buffer.getvalue())\n",
    "    image_request = {\"image_in_base64\": \"{0}\".format(encoded), \"parameters\": parameters}\n",
    "    payload.append(image_request)\n",
    "    body = json.dumps(payload)\n",
    "    r = requests.post(service_endpoint_url, data=body, headers=headers)\n",
    "    try:\n",
    "        result = json.loads(r.text)\n",
    "        json.loads(result[0])\n",
    "    except:\n",
    "        raise ValueError(\"Incorrect output format. Result cant not be parsed: \" + r.text)\n",
    "    return result[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse serialized result from webservice\n",
    "The result from the webserice is in json string. You can parse it the with different DNN model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_or_url = image_path\n",
    "print(\"Image source:\",image_path_or_url)\n",
    "serialized_result_in_json = deploy_obj.score_image(image_path_or_url)\n",
    "print(\"serialized_result_in_json:\", serialized_result_in_json[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse result from json string\n",
    "import numpy as np\n",
    "parsed_result = TFFasterRCNN.parse_serialized_result(serialized_result_in_json)\n",
    "print(\"Parsed result:\", parsed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = detection_utils.visualize(image_path, parsed_result)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_web.jpg\"\n",
    "path_save_dir = os.path.dirname(os.path.abspath(path_save))\n",
    "os.makedirs(path_save_dir, exist_ok=True)\n",
    "ax.get_figure().savefig(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX \n",
    "\n",
    "# (A) Using pretrained model\n",
    "\n",
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_detector_pt = TFFasterRCNN(labels=None, name=\"pretrained\")\n",
    "frozen_model_path, label_map_path = my_detector_pt.init_pretrained(use_frozen=True)\n",
    "print(\"Frozen model written to path: \" + frozen_model_path)\n",
    "print(\"Labels written to path: \" + label_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with using preloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = my_detector_pt.score(image_path)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_pretrained.jpg\"\n",
    "image_size = (8, 12)\n",
    "ax = detection_utils.visualize(image_path, scores, label_map_path, path_save=path_save,\n",
    "                              image_size=image_size)\n",
    "# ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with using frozen graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load detection graph once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = detection_utils.load_graph(frozen_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_dict = detection_utils.score(detection_graph, image_path)\n",
    "path_save = \"../../../cvtk_output/scored_images/scored_image_pretrained_frozen.jpg\"\n",
    "image_size = (8, 12)\n",
    "ax = detection_utils.visualize(image_path, detections_dict, label_map_path, path_save=path_save,\n",
    "                              image_size=image_size)\n",
    "# ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Webcam scoring\n",
    "\n",
    "The code below shows how to read in frames from a webcam (or optionally from disk) and run object detection on them. As detector, a pre-trained COCO model is used, but one can use any trained detector as input instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvtk.core import Context, TFFasterRCNN\n",
    "from cvtk.utils.detection_utils import FilepathImageProvider, VideoImageProvider\n",
    "%matplotlib inline\n",
    "\n",
    "out_root_path = \"../../../cvtk_output\"\n",
    "Context.create(outputs_path=out_root_path, persistent_path=out_root_path, temp_path=out_root_path)\n",
    "\n",
    "# Initialize detector with pre-trained model\n",
    "my_detector = TFFasterRCNN(labels=None, name=\"pretrained\")\n",
    "my_detector.init_pretrained()\n",
    "\n",
    "# Choose image provider\n",
    "# image_provider = VideoImageProvider() # read images from webcam\n",
    "image_provider = FilepathImageProvider([image.storage_path for image in data_val.images])  #read images from disk\n",
    "#image_provider = VideoImageProvider(cv2_video_capture = cv2.VideoCapture(\"movie.mp4\")) #read images from video file\n",
    "\n",
    "# Optionally save visualization to video\n",
    "# video_size = (640, 480)\n",
    "# cv2_video_writer = cv2.VideoWriter('out_video.avi', cv2.VideoWriter_fourcc(*'XVID'), 5.0, video_size)\n",
    "\n",
    "# Run object detection\n",
    "_ = my_detector.score_multiple(image_provider, visualize=True) #, cv2_video_writer = cv2_video_writer, cv2_video_writer_img_size = video_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2018 Microsoft. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
